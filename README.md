# ðŸš¢ Titanic ETL Pipeline

## ðŸ”¹ Overview
This project implements an **ETL (Extract, Transform, Load) pipeline** for the **Titanic dataset**. The goal is to clean, transform, and load data for analysis, making it ready for modeling and visualization.

---

## ðŸ“š Steps Covered
- **Extract:** Load raw Titanic CSV data from local files.  
- **Transform:**  
  - Handle missing values  
  - Convert categorical features into numeric codes  
  - Create derived features (like family size, title, etc.)  
- **Load:** Insert cleaned and transformed data into a database (Supabase/PostgreSQL).  
- **Validate:** Check for nulls, duplicates, and ensure data consistency.  
- **Analyze:** Generate summary statistics and visualizations like survival rates, passenger class distributions, and age histograms.  

---

## ðŸ’» Code / File Structure
ETL-Titanic/
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Original CSV files
â”‚ â”œâ”€â”€ staged/ # Transformed data
â”‚ â””â”€â”€ processed/ # Analysis outputs
â”œâ”€â”€ extract.py # Extract raw data
â”œâ”€â”€ transform.py # Transform data
â”œâ”€â”€ load.py # Load data into database
â”œâ”€â”€ validate.py # Validate transformed data
â”œâ”€â”€ etl_analysis.py # Generate metrics and visualizations
â””â”€â”€ README.md
